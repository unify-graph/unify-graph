#!/usr/bin/env python3
"""Batch Wikidata QID reconciliation for all entities.

Queries the Wikidata search API for each entity name and outputs
a CUE overlay file (external_ids.cue) with wikidata QIDs.

Usage:
  Local:  .venv/bin/python3 scripts/wikidata_reconcile.py
  CI:     triggered manually via 'wikidata-reconcile' job in .gitlab-ci.yml

IMPORTANT: Always review output diffs before committing.
Wikidata's top-1 search result can be wrong for ambiguous names
(e.g., "George Mitchell" returns a coach, not the senator).
The corrections in scripts/wikidata_qids.json are the reviewed truth.
"""
import json
import sys
import time
import urllib.request
import urllib.parse

API = "https://www.wikidata.org/w/api.php"

UA = "unify-graph/1.0 (https://github.com/unify-graph/unify-graph; entity reconciliation)"

def search_wikidata(name: str, limit: int = 3) -> list[dict]:
    params = urllib.parse.urlencode({
        "action": "wbsearchentities",
        "search": name,
        "language": "en",
        "format": "json",
        "limit": limit,
    })
    url = f"{API}?{params}"
    try:
        req = urllib.request.Request(url, headers={"User-Agent": UA})
        with urllib.request.urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read())
            return data.get("search", [])
    except Exception as e:
        print(f"  ERROR: {e}", file=sys.stderr)
        return []

def pick_best(results: list[dict], name: str) -> dict | None:
    if not results:
        return None
    # Exact label match gets priority
    for r in results:
        if r.get("label", "").lower() == name.lower():
            return r
    # Otherwise first result
    return results[0]

def main():
    # Load entities from CUE export
    with open("site/data/entities.json") as f:
        entities = json.load(f)

    print(f"Reconciling {len(entities)} entities against Wikidata...\n")

    results = {}
    skipped = []
    ambiguous = []

    for key, entity in sorted(entities.items()):
        name = entity["name"]
        sys.stdout.write(f"  {key}: {name} ... ")
        sys.stdout.flush()

        hits = search_wikidata(name)
        time.sleep(0.2)  # Be nice to the API

        best = pick_best(hits, name)
        if best:
            qid = best["id"]
            desc = best.get("description", "")
            results[key] = {"qid": qid, "description": desc}
            print(f"{qid} ({desc})")
        else:
            skipped.append(key)
            print("NOT FOUND")

    # Write JSON mapping
    mapping = {k: v["qid"] for k, v in results.items()}
    with open("scripts/wikidata_qids.json", "w") as f:
        json.dump(mapping, f, indent=2)

    # Write CUE overlay
    with open("external_ids.cue", "w") as f:
        f.write("// External identifiers â€” Wikidata QIDs for entity reconciliation.\n")
        f.write("// Auto-generated by scripts/wikidata_reconcile.py\n")
        f.write("package creeps\n\n")
        f.write("entities: {\n")
        for key in sorted(results.keys()):
            qid = results[key]["qid"]
            f.write(f'\t{key}: external_ids: wikidata: "{qid}"\n')
        f.write("}\n")

    print(f"\nDone: {len(results)} matched, {len(skipped)} skipped")
    print(f"Skipped: {', '.join(skipped)}")
    print(f"\nOutput:")
    print(f"  scripts/wikidata_qids.json  (raw mapping)")
    print(f"  external_ids.cue            (CUE overlay)")

if __name__ == "__main__":
    main()
